---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

[PILOT/COPILOT - TEXT IN SQUARE BRACKETS IS HERE FOR GUIDANCE. COPILOT PLEASE DELETE BEFORE KNITTING THE FINAL REPORT]

# Report Details

[PILOT/COPILOT ENTER RELEVANT REPORT DETAILS HERE]

```{r}
articleID <- "CARPS_EXT_5-2-2016" # insert the article ID code here e.g., "10-3-2015_PS"
reportType <- "pilot" # specify whether this is the 'pilot' report or 'final' report
pilotNames <- "Lauren Borchers" # insert the pilot's name here e.g., "Tom Hardwicke".  If there are multiple cpilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- "Andrew Nam" # # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- "NA" # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- "300" # insert the co-pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- "10/31/18" # insert the pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- "11/04/18" # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- "11/04/18" # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

------

#### Methods summary: 

The authors used a third-party-punishment game. Within this game there was a dictator, a recipient, and a punisher. Dictators received \$10.00 and could choose to allocate some of the money to recipients in increments of \$2.50. Punishers are given a separate sum of \$5.00 and could penalize the dictator for not giving money in \$1.00 increments (penalizing the dictator at 20% per dollar). Recipients cannot do anything. All players first committed to the decisions they would make as the dictator, recipient, and punisher before they were assigned their respective role. This allowed the authors to maximize data collection for all three conditions. Dictators were asked to make a binary decision on different allocations of money. The researchers computed valuation scores by looking at switch-points within the dictator's decision. For example, the dictator could decide to keep \$12.00 rather than giving \$20 and give up \$10.00 to give \$20.00 (valuation score= ((12/20+10/20)/2)=0.55). 

------

#### Target outcomes: 

> Descriptive statistics for punishment, allocations, and valuation. Behavior in our modified third-partypunishment  game resembled behavior in typical versions. As usual, punishers were sometimes willing to punish (Fig. 1). For instance, when assuming that dictators allocated nothing to recipients, 55% of punishers (65 of 119) spent at least \$1 on punishment, and 15% (18 of 119) spent their entire \$5 on punishment. Dictators were sometimes willing to allocate money to recipients (diamonds in Fig. 1). For instance, 64% of dictators (76 of  119) were willing to allocate at least some money to recipients, and 38% (45 of 119) were willing to allocate at least half of the total stake. Even when dictators allocated half or more of their stake to recipients, a minority of punishers were still willing to punish. This might reflect noise, as subjects who did not anticipate such decisions being actualized may have answered with less concern, or it might reflect antisocial punishment, which has been observed in other games (Herrmann, Thöni, & Gächter, 2008; Masclet, Noussair, Tucker, & Villeval, 2003). When dictators allocated all \$10, punishment, though still costly, was completely ineffective; in this case, dictators had no residual endowment that punishment could reduce. Nonetheless, past experiments have also shown that people are willing to pay for costly punishment even when such punishment is completely ineffective (Yamagishi et al., 2009). Dictators moderately valued recipients and punishers (Ms = .22 and .23, respectively). Roughly, a dictator would forgo receiving up to \$4 if recipients and punishers could receive \$20.

> Punishers assume that dictators’ treatment of recipients predicts how much dictators value punishers. As we have shown, dictators’ divisions toward recipients predict how much dictators value punishers. But do third parties actually make inferences using this cue? This is a primary prediction of the deterrence hypothesis, and our results confirmed it: As shown in Figure 2a, the more that dictators allocated to recipients, the more they were thought to value both the punisher and the recipient. In fact, the inferences punishers made for the two targets (themselves and recipients) were indistinguishable. Using a 2 × 2 repeated measures ANOVA with allocation and target as factors, we found that the effect of allocation on inferences was significant, F(2.32, 273.72) = 18.15, p < .001, η2 = .13, but there was no effect of target and no interaction between target and allocation (ps = .25 and .17; see Table S2 in the Supplemental Material).

------

[PILOT/COPILOT DO NOT CHANGE THE CODE IN THE CHUNK BELOW]  

```{r global_options, include=FALSE}
# sets up some formatting options for the R Markdown document
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Step 1: Load packages and prepare report object

[PILOT/COPILOT Some useful packages are being loaded below. You can add any additional ones you might need too.]

```{r}
# load packages
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
```

[PILOT/COPILOT DO NOT MAKE CHANGES TO THE CODE CHUNK BELOW]

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

# Step 2: Load data

```{r}
S1_data <- read_sav("data/S1_data.sav")
```

# Step 3: Tidy data

```{r}
# make a new data frame for punishment data
S1_data_punishments = 
  # where we:
  # go into the original data
  S1_data %>%
  # only select columns with "Punishment" in the name
    select(contains("Punishment"))
```

# Step 4: Run analysis

```{r}
describe_names = function(df) {
  sapply(names(df), function(name) {
    return(attr(df[[name]], "label"))
  })
}
```


```{r}
describe_names(S1_data)
describe_names(S1_data_punishments)

# how much money punishers spent to punish the dictator
# when the dictator gave $0
punishments_when_dict_gave_nothing = S1_data_punishments$PunishmentDecision.1000
# out of all punishers, what percent spent at least one dollar?
punishments_when_dict_gave_nothing
# so, when is this vector greater than or equal to one?
punishments_that_are_at_least_a_dollar = punishments_when_dict_gave_nothing[punishments_when_dict_gave_nothing >= 1]
n_s_that_gave_dollar = length(punishments_that_are_at_least_a_dollar) #65
#119 is original sample
n_s_total = length (punishments_when_dict_gave_nothing)
#Reproduction of 55% gave at least one dollar for punishment
n_s_that_gave_dollar/n_s_total
```

```{r}
#Claim that 18 of 119 spent their entire $5 on punishment
# cases when punishers spent all money to punish the dictator when dictator gave $0
punishments_when_dict_gave_nothing = S1_data_punishments$PunishmentDecision.1000
# cases when vector equals to $5
punishments_entire_five =
punishments_when_dict_gave_nothing[punishments_when_dict_gave_nothing >= 5]
n_s_that_gave_five_dollars = length(punishments_entire_five)
#119 is original sample
n_s_total = length (punishments_when_dict_gave_nothing)
length (punishments_entire_five) #18
n_s_that_gave_five_dollars/n_s_total #true that 18/119 spent entire $5 on punishment
length (punishments_entire_five)
```

```{r}
#64% of dictators (76 of 119) were willing to allocate at least some money to recipients
cases_of_dict = S1_data$TransferByDictator
dict_gave_some_money =
cases_of_dict [cases_of_dict >= 2.50]
n_dict_that_gave_some_money = length(cases_of_dict)
n_dict_gave_some_money= length(dict_gave_some_money)
n_dict_gave_some_money #76
cases_of_dict=length(cases_of_dict) #119
cases_of_dict
n_dict_gave_some_money/cases_of_dict #64% of dicators gave some money
```

```{r}
#38% (45 of 119) were willing to allocate at least half of the total stake
cases_of_dict_half = S1_data$TransferByDictator
dict_gave_half_money =
cases_of_dict_half [cases_of_dict_half >= 5.00]
n_dict_that_gave_half_money = length(cases_of_dict)
n_dict_gave_half_money= length(dict_gave_half_money)
n_dict_gave_half_money #45
cases_of_dict_half=length(cases_of_dict_half) #119
cases_of_dict_half
n_dict_gave_half_money/cases_of_dict_half #38% of dicators gave some money
```

```{r}
#Dictators moderately valued recipients and punishers (Ms = .22 and .23, respectively). Roughly, a dictator would forgo receiving up to $4 if recipients and punishers could receive $20.
data(S1_data)
mean(S1_data[["WTR.RECIPIENT.AsDictator"]]) #dictator's value of recipients 
data(S1_data)
mean(S1_data[["WTR.RESPONDER.AsDictator"]]) #dictator's value of punishers 
```

```{r}
#the more that dictators allocated to recipients, the more they were thought to value both the punisher and the recipient. In fact, the 
#inferences punishers made for the two targets (themselves and recipients) were indistinguishable. 
#Using a 2 × 2 repeated measures ANOVA with 
#allocation and target as factors, 
#we found that the effect of allocation on inferences was significant, F(2.32, 273.72) = 18.15, p < .001, η2 = .13, 
#no effect of target and no interaction between target and allocation (ps = .25 and .17; see Table S2 in the Supplemental Material).

#all testing below. Haven't reproduced result
res.aov2 <- aov(WTR.RESPONDER.AsDictator ~ TransferByDictator + TransferEstimate, data = S1_data)
summary(res.aov2)


res.aov2 <- aov(WTR.RESPONDER.AsDictator ~TransferEstimate +TransferByDictator, data = S1_data)
summary(res.aov2)

#FirstHypotheticalTransfer

res.aov2 <- aov(WTR.RECIPIENT.AsDictator ~TransferEstimate +TransferByDictator, data = S1_data)
summary(res.aov2)

#trying this out
res.aov2 <- aov(WTR.RESPONDER.AsDictator ~ TransferByDictator + TransferEstimate, data = S1_data)
summary(res.aov2)
```

## Descriptive statistics

```{r}
```

## Inferential statistics

```{r}
```

# Step 5: Conclusion

[Please include a text summary describing your findings. If this reproducibility check was a failure, you should note any suggestions as to what you think the likely cause(s) might be.]
  
[PILOT/COPILOT ENTER RELEVANT INFORMATION BELOW]

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)

Insufficient_Information_Errors <- 0 # how many discrete insufficient information issues did you encounter?

# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).

locus_typo <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- NA # how many discrete issues were there for which you could not identify the cause

# How many of the above issues were resolved through author assistance?
locus_typo_resolved <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification_resolved <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis_resolved <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data_resolved <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified_resolved <- NA # how many discrete issues were there for which you could not identify the cause

Affects_Conclusion <- NA # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? TRUE, FALSE, or NA. This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```

[PILOT/COPILOT DOD NOT EDIT THE CODE CHUNK BELOW]

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add variables to report 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome %in% c("MAJOR_ERROR", "DECISION_ERROR")) | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified, locus_typo_resolved, locus_specification_resolved, locus_analysis_resolved, locus_data_resolved, locus_unidentified_resolved)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

# Session information

[This function will output information about the package versions used in this report:]

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
